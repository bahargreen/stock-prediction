{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xos0XZ6nLkWC"
   },
   "outputs": [],
   "source": [
    "#Standard libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "# sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Keras\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkbqmQtEMDkF"
   },
   "outputs": [],
   "source": [
    "# Define a callback class\n",
    "# Resets the states after each epoch (after going through a full time series)\n",
    "class ModelStateReset(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.model.reset_states()\n",
    "reset=ModelStateReset()\n",
    "\n",
    "# Different Approach\n",
    "#class modLSTM(LSTM):\n",
    "#    def call(self, x, mask=None):\n",
    "#        if self.stateful: \n",
    "#             self.reset_states()\n",
    "#        return super(modLSTM, self).call(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRt5uLClMKxd"
   },
   "outputs": [],
   "source": [
    "# Function to create an LSTM model, required for KerasClassifier\n",
    "def create_shallow_LSTM(epochs=1, \n",
    "                        LSTM_units=1,\n",
    "                        num_samples=1, \n",
    "                        look_back=1,\n",
    "                        num_features=None,  \n",
    "                        dropout_rate=0,\n",
    "                        recurrent_dropout=0,\n",
    "                        verbose=0):\n",
    "    \n",
    "    model=Sequential()\n",
    "    \n",
    "    model.add(LSTM(units=LSTM_units, \n",
    "                   batch_input_shape=(num_samples, look_back, num_features), \n",
    "                   stateful=True, \n",
    "                   recurrent_dropout=recurrent_dropout)) \n",
    "    \n",
    "    model.add(Dropout(dropout_rate))\n",
    "            \n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer=keras.initializers.he_normal(seed=1)))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8pQmMw8MYvC"
   },
   "outputs": [],
   "source": [
    "stocks = pd.read_csv(\"http://s3.amazonaws.com/hr-testcases-us-east-1/329/assets/bigger_data_set.txt\", sep=\" \") \n",
    "s=stocks.T\n",
    "df = s.reset_index(level=0)\n",
    "df.columns = df.iloc[0] \n",
    "\n",
    "df = df[1:]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BRdfP7kLMoj3"
   },
   "outputs": [],
   "source": [
    "y=df['UCB'].apply(lambda x: float(x))\n",
    "for i in range(506):\n",
    "  day1.append(i)\n",
    "x=day1[1:]\n",
    "print(len(x))\n",
    "print(len(y))\n",
    "\n",
    "dataset = pd.DataFrame({'day': x, 'prices': list(y)}, columns=['day', 'prices'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q5CGfaBoQaYU",
    "outputId": "baf0978d-9651-478c-f2a7-def31bff69eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:88: FutureWarning: In a future version of pandas all arguments of DataFrame.dropna will be keyword-only\n"
     ]
    }
   ],
   "source": [
    "# Compute the logarithmic returns using the  price \n",
    "sp500 = pd.DataFrame()\n",
    "\n",
    "sp500['Volume']=dataset['prices']\n",
    "sp500['Log_Ret_1d']=np.log(dataset['prices'] / dataset['prices'].shift(1))\n",
    "\n",
    "# Compute logarithmic returns using the pandas rolling mean function\n",
    "sp500['Log_Ret_1w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=5).sum()\n",
    "sp500['Log_Ret_2w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=10).sum()\n",
    "sp500['Log_Ret_3w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=15).sum()\n",
    "sp500['Log_Ret_4w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=20).sum()\n",
    "sp500['Log_Ret_8w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=40).sum()\n",
    "sp500['Log_Ret_12w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=60).sum()\n",
    "sp500['Log_Ret_16w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=80).sum()\n",
    "sp500['Log_Ret_20w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=100).sum()\n",
    "sp500['Log_Ret_24w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=120).sum()\n",
    "sp500['Log_Ret_28w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=140).sum()\n",
    "sp500['Log_Ret_32w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=160).sum()\n",
    "sp500['Log_Ret_36w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=180).sum()\n",
    "sp500['Log_Ret_40w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=200).sum()\n",
    "sp500['Log_Ret_44w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=220).sum()\n",
    "sp500['Log_Ret_48w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=240).sum()\n",
    "sp500['Log_Ret_52w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=260).sum()\n",
    "sp500['Log_Ret_56w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=280).sum()\n",
    "sp500['Log_Ret_60w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=300).sum()\n",
    "sp500['Log_Ret_64w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=320).sum()\n",
    "sp500['Log_Ret_68w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=340).sum()\n",
    "sp500['Log_Ret_72w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=360).sum()\n",
    "sp500['Log_Ret_76w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=380).sum()\n",
    "sp500['Log_Ret_80w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=400).sum()\n",
    "\n",
    "# Compute Volatility using the pandas rolling standard deviation function\n",
    "sp500['Vol_1w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=5).std()*np.sqrt(5)\n",
    "sp500['Vol_2w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=10).std()*np.sqrt(10)\n",
    "sp500['Vol_3w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=15).std()*np.sqrt(15)\n",
    "sp500['Vol_4w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=20).std()*np.sqrt(20)\n",
    "sp500['Vol_8w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=40).std()*np.sqrt(40)\n",
    "sp500['Vol_12w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=60).std()*np.sqrt(60)\n",
    "sp500['Vol_16w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=80).std()*np.sqrt(80)\n",
    "sp500['Vol_20w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=100).std()*np.sqrt(100)\n",
    "sp500['Vol_24w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=120).std()*np.sqrt(120)\n",
    "sp500['Vol_28w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=140).std()*np.sqrt(140)\n",
    "sp500['Vol_32w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=160).std()*np.sqrt(160)\n",
    "sp500['Vol_36w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=180).std()*np.sqrt(180)\n",
    "sp500['Vol_40w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=200).std()*np.sqrt(200)\n",
    "sp500['Vol_44w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=220).std()*np.sqrt(220)\n",
    "sp500['Vol_48w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=240).std()*np.sqrt(240)\n",
    "sp500['Vol_52w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=260).std()*np.sqrt(260)\n",
    "sp500['Vol_56w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=280).std()*np.sqrt(280)\n",
    "sp500['Vol_60w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=300).std()*np.sqrt(300)\n",
    "sp500['Vol_64w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=320).std()*np.sqrt(320)\n",
    "sp500['Vol_68w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=340).std()*np.sqrt(340)\n",
    "sp500['Vol_72w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=360).std()*np.sqrt(360)\n",
    "sp500['Vol_76w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=380).std()*np.sqrt(380)\n",
    "sp500['Vol_80w']=pd.Series(sp500['Log_Ret_1d']).rolling(window=400).std()*np.sqrt(400)\n",
    "\n",
    "\n",
    "# Compute Volumes using the pandas rolling mean function\n",
    "sp500['Volume_1w']=pd.Series(sp500['Volume']).rolling(window=5).mean()\n",
    "sp500['Volume_2w']=pd.Series(sp500['Volume']).rolling(window=10).mean()\n",
    "sp500['Volume_3w']=pd.Series(sp500['Volume']).rolling(window=15).mean()\n",
    "sp500['Volume_4w']=pd.Series(sp500['Volume']).rolling(window=20).mean()\n",
    "sp500['Volume_8w']=pd.Series(sp500['Volume']).rolling(window=40).mean()\n",
    "sp500['Volume_12w']=pd.Series(sp500['Volume']).rolling(window=60).mean()\n",
    "sp500['Volume_16w']=pd.Series(sp500['Volume']).rolling(window=80).mean()\n",
    "sp500['Volume_20w']=pd.Series(sp500['Volume']).rolling(window=100).mean()\n",
    "sp500['Volume_24w']=pd.Series(sp500['Volume']).rolling(window=120).mean()\n",
    "sp500['Volume_28w']=pd.Series(sp500['Volume']).rolling(window=140).mean()\n",
    "sp500['Volume_32w']=pd.Series(sp500['Volume']).rolling(window=160).mean()\n",
    "sp500['Volume_36w']=pd.Series(sp500['Volume']).rolling(window=180).mean()\n",
    "sp500['Volume_40w']=pd.Series(sp500['Volume']).rolling(window=200).mean()\n",
    "sp500['Volume_44w']=pd.Series(sp500['Volume']).rolling(window=220).mean()\n",
    "sp500['Volume_48w']=pd.Series(sp500['Volume']).rolling(window=240).mean()\n",
    "sp500['Volume_52w']=pd.Series(sp500['Volume']).rolling(window=260).mean()\n",
    "sp500['Volume_56w']=pd.Series(sp500['Volume']).rolling(window=280).mean()\n",
    "sp500['Volume_60w']=pd.Series(sp500['Volume']).rolling(window=300).mean()\n",
    "sp500['Volume_64w']=pd.Series(sp500['Volume']).rolling(window=320).mean()\n",
    "sp500['Volume_68w']=pd.Series(sp500['Volume']).rolling(window=340).mean()\n",
    "sp500['Volume_72w']=pd.Series(sp500['Volume']).rolling(window=360).mean()\n",
    "sp500['Volume_76w']=pd.Series(sp500['Volume']).rolling(window=380).mean()\n",
    "sp500['Volume_80w']=pd.Series(sp500['Volume']).rolling(window=400).mean()\n",
    "\n",
    "# Label data: Up (Down) if the the 1 month (≈ 21 trading days) logarithmic return increased (decreased)\n",
    "sp500['Return_Label']=pd.Series(sp500['Log_Ret_1d']).shift(-21).rolling(window=21).sum()\n",
    "sp500['Label']=np.where(sp500['Return_Label'] > 0, 1, 0)\n",
    "\n",
    "# Drop NA´s\n",
    "sp500=sp500.dropna(\"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1zSSOfRSla5"
   },
   "outputs": [],
   "source": [
    "# Show rows and columns\n",
    "print(\"Rows, Columns:\");print(sp500.shape);print(\"\\n\")\n",
    "\n",
    "# Describe DataFrame columns\n",
    "print(\"Columns:\");print(sp500.columns);print(\"\\n\")\n",
    "\n",
    "# Show info on DataFrame\n",
    "print(\"Info:\");print(sp500.info()); print(\"\\n\")\n",
    "\n",
    "# Count Non-NA values\n",
    "print(\"Non-NA:\");print(sp500.count()); print(\"\\n\")\n",
    "\n",
    "# Show head\n",
    "print(\"Head\");print(sp500.head()); print(\"\\n\")\n",
    "\n",
    "# Show tail\n",
    "print(\"Tail\");print(sp500.tail());print(\"\\n\")\n",
    "\n",
    "# Show summary statistics\n",
    "print(\"Summary statistics:\");print(sp500.describe());print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9rYZJLrKXq3V"
   },
   "outputs": [],
   "source": [
    "# Plot the logarithmic returns\n",
    "sp500.iloc[:,1:24].plot(subplots=True, color='blue', figsize=(20, 20));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhdmPeknX8sr"
   },
   "outputs": [],
   "source": [
    "# Plot correlation matrix\n",
    "\n",
    "focus_cols=sp500.iloc[:,24:47].columns \n",
    "\n",
    "corr=sp500.iloc[:,24:70].corr().filter(focus_cols).drop(focus_cols)\n",
    "\n",
    "mask=np.zeros_like(corr); mask[np.triu_indices_from(mask)]=True # we use mask to plot only part of the matrix\n",
    "\n",
    "heat_fig, (ax)=plt.subplots(1, 1, figsize=(9,6))\n",
    "\n",
    "heat=sns.heatmap(corr, \n",
    "                   ax=ax, \n",
    "                   mask=mask, \n",
    "                   vmax=.5, \n",
    "                   square=True, \n",
    "                   linewidths=.2, \n",
    "                   cmap=\"Blues_r\")\n",
    "\n",
    "heat_fig.subplots_adjust(top=.93)\n",
    "\n",
    "heat_fig.suptitle('Volatility vs. Volume', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.savefig('heat1.eps', dpi=200, format='eps');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9iW-gp4sYRVX"
   },
   "outputs": [],
   "source": [
    "# Model Set 1: Volatility\n",
    "\n",
    "# Baseline\n",
    "X_train_1, X_test_1, y_train_1, y_test_1=train_test_split(sp500.iloc[:,24:47], sp500.iloc[:,72], test_size=0.1 ,shuffle=False, stratify=None)\n",
    "# LSTM \n",
    "# Input arrays should be shaped as (samples or batch, time_steps or look_back, num_features):\n",
    "X_train_1_lstm=X_train_1.values.reshape(X_train_1.shape[0], 1, X_train_1.shape[1])\n",
    "X_test_1_lstm=X_test_1.values.reshape(X_test_1.shape[0], 1, X_test_1.shape[1])\n",
    "\n",
    "# Model Set 2: Return\n",
    "X_train_2, X_test_2, y_train_2, y_test_2=train_test_split(sp500.iloc[:,1:24], sp500.iloc[:,72], test_size=0.1 ,shuffle=False, stratify=None)\n",
    "# LSTM \n",
    "# Input arrays should be shaped as (samples or batch, time_steps or look_back, num_features):\n",
    "X_train_2_lstm=X_train_2.values.reshape(X_train_2.shape[0], 1, X_train_2.shape[1])\n",
    "X_test_2_lstm=X_test_2.values.reshape(X_test_2.shape[0], 1, X_test_2.shape[1])\n",
    "\n",
    "# Model Set 3: Volume\n",
    "X_train_3, X_test_3, y_train_3, y_test_3=train_test_split(sp500.iloc[:,47:72], sp500.iloc[:,72], test_size=0.1 ,shuffle=False, stratify=None)\n",
    "# LSTM \n",
    "# Input arrays should be shaped as (samples or batch, time_steps or look_back, num_features):\n",
    "X_train_3_lstm=X_train_3.values.reshape(X_train_3.shape[0], 1, X_train_3.shape[1])\n",
    "X_test_3_lstm=X_test_3.values.reshape(X_test_3.shape[0], 1, X_test_3.shape[1])\n",
    "\n",
    "# Model Set 4: Volatility and Return\n",
    "X_train_4, X_test_4, y_train_4, y_test_4=train_test_split(sp500.iloc[:,1:47], sp500.iloc[:,72], test_size=0.1 ,shuffle=False, stratify=None)\n",
    "# LSTM \n",
    "# Input arrays should be shaped as (samples or batch, time_steps or look_back, num_features):\n",
    "X_train_4_lstm=X_train_4.values.reshape(X_train_4.shape[0], 1, X_train_4.shape[1])\n",
    "X_test_4_lstm=X_test_4.values.reshape(X_test_4.shape[0], 1, X_test_4.shape[1])\n",
    "\n",
    "# Model Set 5: Volatility and Volume\n",
    "X_train_5, X_test_5, y_train_5, y_test_5=train_test_split(sp500.iloc[:,24:72], sp500.iloc[:,72], test_size=0.1 ,shuffle=False, stratify=None)\n",
    "# LSTM \n",
    "# Input arrays should be shaped as (samples or batch, time_steps or look_back, num_features):\n",
    "X_train_5_lstm=X_train_5.values.reshape(X_train_5.shape[0], 1, X_train_5.shape[1])\n",
    "X_test_5_lstm=X_test_5.values.reshape(X_test_5.shape[0], 1, X_test_5.shape[1])\n",
    "\n",
    "# Model Set 6: Return and Volume\n",
    "X_train_6, X_test_6, y_train_6, y_test_6=train_test_split(pd.concat([sp500.iloc[:,1:24], sp500.iloc[:,47:72]], axis=1), sp500.iloc[:,70], test_size=0.1 ,shuffle=False, stratify=None)\n",
    "# LSTM \n",
    "# Input arrays should be shaped as (samples or batch, time_steps or look_back, num_features):\n",
    "X_train_6_lstm=X_train_6.values.reshape(X_train_6.shape[0], 1, X_train_6.shape[1])\n",
    "X_test_6_lstm=X_test_6.values.reshape(X_test_6.shape[0], 1, X_test_6.shape[1])\n",
    "\n",
    "# Model Set 7: Volatility, Return and Volume\n",
    "X_train_7, X_test_7, y_train_7, y_test_7=train_test_split(sp500.iloc[:,1:70], sp500.iloc[:,72], test_size=0.1 ,shuffle=False, stratify=None)\n",
    "# LSTM \n",
    "# Input arrays should be shaped as (samples or batch, time_steps or look_back, num_features):\n",
    "X_train_7_lstm=X_train_7.values.reshape(X_train_7.shape[0], 1, X_train_7.shape[1])\n",
    "X_test_7_lstm=X_test_7.values.reshape(X_test_7.shape[0], 1, X_test_7.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IP_MqmOwYY5K"
   },
   "outputs": [],
   "source": [
    "print(\"train set increase bias = \"+ str(np.mean(y_train_7==1))+\"%\")\n",
    "\n",
    "print(\"test set increase bias = \" + str(np.mean(y_test_7==1))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2i2lxtmZUs4"
   },
   "outputs": [],
   "source": [
    "# Standardized Data\n",
    "steps_b=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), \n",
    "           ('logistic', linear_model.SGDClassifier(loss=\"log\", shuffle=False, early_stopping=False, tol=1e-3, random_state=1))]\n",
    "\n",
    "#Normalized Data\n",
    "#steps_b=[('scaler', MinMaxScaler(feature_range=(0, 1), copy=True)), \n",
    "#         ('logistic', linear_model.SGDClassifier(loss=\"log\", shuffle=False, early_stopping=False, tol=1e-3, random_state=1))]\n",
    "\n",
    "pipeline_b=Pipeline(steps_b) # Using a pipeline we glue together the Scaler & the Classifier\n",
    "# This ensure that during cross validation the Scaler is fitted to only the training folds\n",
    "\n",
    "# Penalties\n",
    "penalty_b=['l1', 'l2', 'elasticnet']\n",
    "\n",
    "# Evaluation Metric\n",
    "scoring_b={'AUC': 'roc_auc', 'accuracy': make_scorer(accuracy_score)} #multiple evaluation metrics\n",
    "metric_b='accuracy' #scorer is used to find the best parameters for refitting the estimator at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_iWWB2LZjOV"
   },
   "outputs": [],
   "source": [
    "# Batch_input_shape=[1, 1, Z]  -> (batch size, time steps, number of features) \n",
    "# Data set inputs(trainX)=[X, 1, Z]  -> (samples, time steps, number of features)  \n",
    "\n",
    "# number of samples\n",
    "num_samples=1 \n",
    "# time_steps\n",
    "look_back=1\n",
    "\n",
    "\n",
    "# Evaluation Metric\n",
    "scoring_lstm='accuracy' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G98mpIsTaRn0"
   },
   "outputs": [],
   "source": [
    "# Time Series Split \n",
    "dev_size=0.1 \n",
    "n_splits=int((1//dev_size)-1)   # using // for integer division\n",
    "tscv=TimeSeriesSplit(n_splits=n_splits) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RF-DLVFhZsBn"
   },
   "outputs": [],
   "source": [
    "# Model specific Parameter \n",
    "\n",
    "# Number of iterations\n",
    "iterations_1_b=[8] \n",
    "\n",
    "\n",
    "# Grid Search\n",
    "\n",
    "# Regularization  \n",
    "alpha_g_1_b=[0.0011, 0.0013, 0.0014] #0.0011, 0.0012, 0.0013\n",
    "l1_ratio_g_1_b=[0, 0.2, 0.4, 0.6, 0.8, 1] \n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters_g_1_b={'logistic__alpha':alpha_g_1_b, \n",
    "                       'logistic__l1_ratio':l1_ratio_g_1_b, \n",
    "                       'logistic__penalty':penalty_b,  \n",
    "                       'logistic__max_iter':iterations_1_b}\n",
    "\n",
    "# Create grid search \n",
    "search_g_1_b=GridSearchCV(estimator=pipeline_b, \n",
    "                            param_grid=hyperparameters_g_1_b, \n",
    "                            cv=tscv, \n",
    "                            verbose=0, \n",
    "                            n_jobs=-1, \n",
    "                            scoring=scoring_b, \n",
    "                            refit=metric_b, \n",
    "                            return_train_score=False)\n",
    "# Setting refit='Accuracy', refits an estimator on the whole dataset with the parameter setting that has the best cross-validated mean Accuracy score. \n",
    "# For multiple metric evaluation, this needs to be a string denoting the scorer is used to find the best parameters for refitting the estimator at the end\n",
    "# If return_train_score=True training results of CV will be saved as well \n",
    "\n",
    "# Fit grid search\n",
    "tuned_model_1_b=search_g_1_b.fit(X_train_1, y_train_1)\n",
    "#search_g_1_b.cv_results_\n",
    "\n",
    "\n",
    "# Random Search\n",
    "\n",
    "# Create regularization hyperparameter distribution using uniform distribution\n",
    "#alpha_r_1_b=uniform(loc=0.00006, scale=0.002) \n",
    "#l1_ratio_r_1_b=uniform(loc=0, scale=1) \n",
    "\n",
    "# Create hyperparameter options\n",
    "#hyperparameters_r_1_b={'logistic__alpha':alpha_r_1_b, 'logistic__l1_ratio':l1_ratio_r_1_b, 'logistic__penalty':penalty_b,'logistic__max_iter':iterations_1_b}\n",
    "\n",
    "# Create randomized search \n",
    "#search_r_1_b=RandomizedSearchCV(pipeline_b, \n",
    "#                                  hyperparameters_r_1_b, \n",
    "#                                  n_iter=10, \n",
    "#                                  random_state=1, \n",
    "#                                  cv=tscv, \n",
    "#                                  verbose=0, \n",
    "#                                  n_jobs=-1, \n",
    "#                                  scoring=scoring_b, \n",
    "#                                  refit=metric_b, \n",
    "#                                  return_train_score=True)\n",
    "# Setting refit='Accuracy', refits an estimator on the whole dataset with the parameter setting that has the best cross-validated Accuracy score.\n",
    "# For multiple metric evaluation, this needs to be a string denoting the scorer is used to find the best parameters for refitting the estimator at the end\n",
    "# If return_train_score=True training results of CV will be saved as well  \n",
    "\n",
    "# Fit randomized search\n",
    "#tuned_model_1_b=search_r_1_b.fit(X_train_1, y_train_1)\n",
    "\n",
    "\n",
    "\n",
    "# View Cost function\n",
    "print('Loss function:', tuned_model_1_b.best_estimator_.get_params()['logistic__loss'])\n",
    "\n",
    "# View Accuracy \n",
    "print(metric_b +' of the best model: ', tuned_model_1_b.best_score_);print(\"\\n\")\n",
    "# best_score_ Mean cross-validated score of the best_estimator\n",
    "\n",
    "# View best hyperparameters\n",
    "print(\"Best hyperparameters:\")\n",
    "print('Number of iterations:', tuned_model_1_b.best_estimator_.get_params()['logistic__max_iter'])\n",
    "print('Penalty:', tuned_model_1_b.best_estimator_.get_params()['logistic__penalty'])\n",
    "print('Alpha:', tuned_model_1_b.best_estimator_.get_params()['logistic__alpha'])\n",
    "print('l1_ratio:', tuned_model_1_b.best_estimator_.get_params()['logistic__l1_ratio'])\n",
    "\n",
    "# Find the number of nonzero coefficients (selected features)\n",
    "print(\"Total number of features:\", len(tuned_model_1_b.best_estimator_.steps[1][1].coef_[0][:]))\n",
    "print(\"Number of selected features:\", np.count_nonzero(tuned_model_1_b.best_estimator_.steps[1][1].coef_[0][:]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwZwhG31dwB8"
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_1_b=tuned_model_1_b.predict(X_test_1)\n",
    "\n",
    "# create confustion matrix\n",
    "fig, ax=plt.subplots()\n",
    "sns.heatmap(pd.DataFrame(metrics.confusion_matrix(y_test_1, y_pred_1_b)), annot=True, cmap=\"Blues\" ,fmt='g')\n",
    "plt.title('Confusion matrix'); plt.ylabel('Actual label'); plt.xlabel('Predicted label')\n",
    "ax.xaxis.set_ticklabels(['Down', 'Up']); ax.yaxis.set_ticklabels(['Down', 'Up'])\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test_1, y_pred_1_b))\n",
    "print(\"Precision:\",metrics.precision_score(y_test_1, y_pred_1_b))\n",
    "print(\"Recall:\",metrics.recall_score(y_test_1, y_pred_1_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ElhO6yLd7Qq"
   },
   "outputs": [],
   "source": [
    "y_proba_1_b=tuned_model_1_b.predict_proba(X_test_1)[:, 1]\n",
    "fpr, tpr, _=metrics.roc_curve(y_test_1,  y_proba_1_b)\n",
    "auc=metrics.roc_auc_score(y_test_1, y_proba_1_b)\n",
    "plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--') # plot no skill\n",
    "plt.title('ROC-Curve')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
